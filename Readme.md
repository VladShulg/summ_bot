# Telegram-бот для суммаризации сообщений в групповых чатах

## Описание проекта
Этот проект представляет собой Telegram-бота, который анализирует чаты, создает краткие сводки сообщений с использованием модели T5 и выполняет Named Entity Recognition (NER) с помощью библиотеки Natasha.

## Функциональность
- **Сохранение сообщений пользователей** в чатах с последующим анализом.
- **Генерация краткого содержания сообщений** с использованием модели T5.
- **Обнаружение именованных сущностей (NER)**: определение персон, организаций, локаций и дат.

## Используемые технологии
- **Python 3.12**
- **Aiogram** – для работы с Telegram API. Выбор пал на Aiogram, так как это высокоэффективная и асинхронная библиотека, которая позволяет создавать быстрые и масштабируемые Telegram-боты с минимальной задержкой. Это особенно важно для работы с большим количеством сообщений.
- **Transformers (Hugging Face)** – для суммаризации текста с использованием модели T5 (rut5-base-absum). Выбор модели T5 обусловлен её универсальностью для задач суммаризации текста и простотой интеграции с библиотекой transformers. Модель rut5-base-absum является хорошо обученной на русском языке, что делает её подходящей для обработки русскоязычных сообщений.
- **Natasha** – для Named Entity Recognition (NER). Natasha выбрана, так как это мощный инструмент для обработки русского языка с поддержкой сегментации текста и распознавания именованных сущностей (например, персон, организаций, локаций и дат), что помогает эффективно анализировать и структурировать информацию в сообщениях.
- **Regex** – для предобработки текста

## Технические характеристики модели `cointegrated/rut5-base-absum`

Модель `cointegrated/rut5-base-absum` является предобученной моделью T5 для суммаризации текста на русском языке, созданной на основе архитектуры T5, которая была предложена в 2019 году исследователями Google. Эта модель предназначена для различных задач обработки естественного языка, включая суммаризацию, и является улучшенной версией модели T5, адаптированной для русского языка.

### Основные характеристики:
- **Архитектура**: T5 (Text-to-Text Transfer Transformer).
- **Размер**: Базовая версия модели, которая включает около 220 миллионов параметров.
- **Вес модели**: Модель весит примерно **1.4 GB**.
- **Предобучение**: Модель была обучена на большом количестве текстов на русском языке, что делает её особенно эффективной для обработки русскоязычных текстов.
- **Задачи**: Основной задачей модели является генерация кратких резюме текста, что делает её подходящей для задач суммаризации.
- **Область применения**: Модель используется для создания кратких сводок сообщений, новостей, документов и других текстов.

### Особенности:
- **Поддержка русского языка**: Модель хорошо адаптирована для обработки русского языка, что делает её идеальной для применения в приложениях, ориентированных на русскоязычных пользователей.
- **Скорость и производительность**: Модель обладает хорошими показателями производительности при работе с текстами средней и большой длины. В отличие от более крупных моделей, эта версия обладает хорошей балансировкой между точностью и эффективностью.

Модель `cointegrated/rut5-base-absum` доступна через библиотеку `Transformers` от Hugging Face и может быть использована для решения задач, связанных с суммаризацией текстов, что является основным применением в вашем Telegram-боте.

## Установка и запуск
### 1. Установка зависимостей
```bash
pip install -r requirements.txt
```

### 2. Запуск бота
```bash
python bot.py
```

## Команды бота
- **/start** – Начать работу с ботом.
- **/summary** – Сгенерировать сводку по сообщениям в чате.
- **/set_limits** – Установить ограничение по сообщениям.

### Скринкаст работы также находится в репозитории

## Возможные улучшения
- **Оптимизация хранения сообщений** – использование базы данных вместо `deque`.
- **Добавление фильтрации сообщений** – учитывать только осмысленные сообщения без спама и повторов.
- **Расширение набора сущностей** – добавить анализ других типов информации (например, ключевых слов).

## Автор
Разработчик: **Шульгин Владислав**
